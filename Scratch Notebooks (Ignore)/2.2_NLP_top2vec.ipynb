{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajozC99tlixs",
        "outputId": "5237e88e-d020-47a2-c410-8399b107f7c4"
      },
      "source": [
        "!pip install top2vec\n",
        "!pip install llvmlite==0.34"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: top2vec in /usr/local/lib/python3.6/dist-packages (1.0.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from top2vec) (1.18.5)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.6/dist-packages (from top2vec) (0.8.26)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from top2vec) (1.5.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from top2vec) (3.6.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (from top2vec) (0.4.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from top2vec) (1.1.4)\n",
            "Requirement already satisfied: pynndescent>=0.4 in /usr/local/lib/python3.6/dist-packages (from top2vec) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from hdbscan->top2vec) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from hdbscan->top2vec) (0.17.0)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from hdbscan->top2vec) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hdbscan->top2vec) (1.15.0)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.6/dist-packages (from hdbscan->top2vec) (0.29.21)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud->top2vec) (7.0.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->top2vec) (3.0.0)\n",
            "Requirement already satisfied: numba!=0.47,>=0.46 in /usr/local/lib/python3.6/dist-packages (from umap-learn->top2vec) (0.51.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->top2vec) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->top2vec) (2.8.1)\n",
            "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.6/dist-packages (from pynndescent>=0.4->top2vec) (0.34.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->top2vec) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba!=0.47,>=0.46->umap-learn->top2vec) (50.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->top2vec) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->top2vec) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->top2vec) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->top2vec) (2.10)\n",
            "Requirement already satisfied: llvmlite==0.34 in /usr/local/lib/python3.6/dist-packages (0.34.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDGrGfZkMqq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict \n",
        "from ast import literal_eval\n",
        "from collections import Counter\n",
        "import re\n",
        "import unicodedata\n",
        "# from nlp_preprocessing import *\n",
        "# from topic_modeling import *\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS, CountVectorizer\n",
        "import spacy\n",
        "from top2vec import Top2Vec\n",
        "\n",
        "sp_nlp = spacy.load('en')\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 50)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "pd.reset_option('display.max_colwidth')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3a5e01ukgNX",
        "outputId": "b3a9d563-fd2c-4fcc-f8bf-003992c67be6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "Ye4r6pwLkt-d",
        "outputId": "2849809d-782a-4e9e-9330-b9ab46928642"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Unbiased_news/Data/expanded_in_process.csv\")\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>number</th>\n",
              "      <th>global_bias</th>\n",
              "      <th>title</th>\n",
              "      <th>news_source</th>\n",
              "      <th>text_ascii</th>\n",
              "      <th>text_paras</th>\n",
              "      <th>para_count</th>\n",
              "      <th>text_paras_lemma</th>\n",
              "      <th>text_paras_misc_clean</th>\n",
              "      <th>text_paras_stopwords</th>\n",
              "      <th>text_paras_no_small_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>From the Left</td>\n",
              "      <td>Trump Administration Drops Citizenship Questio...</td>\n",
              "      <td>HuffPost</td>\n",
              "      <td>A very sad time for America when the Supreme C...</td>\n",
              "      <td>A very sad time for America when the Supreme C...</td>\n",
              "      <td>0</td>\n",
              "      <td>a very sad time for America when the Supreme C...</td>\n",
              "      <td>a very sad time for America when the Supreme C...</td>\n",
              "      <td>sad time America Supreme Court United States a...</td>\n",
              "      <td>sad time America Supreme Court United States a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>From the Left</td>\n",
              "      <td>Trump Administration Drops Citizenship Questio...</td>\n",
              "      <td>HuffPost</td>\n",
              "      <td>A very sad time for America when the Supreme C...</td>\n",
              "      <td>President Donald Trump spoke out Tuesday on hi...</td>\n",
              "      <td>1</td>\n",
              "      <td>President Donald Trump speak out Tuesday on  a...</td>\n",
              "      <td>President Donald Trump speak out Tuesday on  a...</td>\n",
              "      <td>President Donald Trump speak Tuesday administr...</td>\n",
              "      <td>President Donald Trump speak Tuesday administr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>From the Left</td>\n",
              "      <td>Trump Administration Drops Citizenship Questio...</td>\n",
              "      <td>HuffPost</td>\n",
              "      <td>A very sad time for America when the Supreme C...</td>\n",
              "      <td>A very sad time for America when the Supreme C...</td>\n",
              "      <td>2</td>\n",
              "      <td>a very sad time for America when the Supreme C...</td>\n",
              "      <td>a very sad time for America when the Supreme C...</td>\n",
              "      <td>sad time America Supreme Court United States a...</td>\n",
              "      <td>sad time America Supreme Court United States a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>From the Left</td>\n",
              "      <td>Trump Administration Drops Citizenship Questio...</td>\n",
              "      <td>HuffPost</td>\n",
              "      <td>A very sad time for America when the Supreme C...</td>\n",
              "      <td>....to do whatever is necessary to bring this ...</td>\n",
              "      <td>3</td>\n",
              "      <td>.... to do whatever be necessary to bring this...</td>\n",
              "      <td>to do whatever be necessary to bring this mos...</td>\n",
              "      <td>whatever necessary bring vital question import...</td>\n",
              "      <td>whatever necessary bring vital question import...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>From the Left</td>\n",
              "      <td>Trump Administration Drops Citizenship Questio...</td>\n",
              "      <td>HuffPost</td>\n",
              "      <td>A very sad time for America when the Supreme C...</td>\n",
              "      <td>Commerce Secretary Wilbur Ross, who oversees t...</td>\n",
              "      <td>4</td>\n",
              "      <td>Commerce Secretary Wilbur Ross , who oversee t...</td>\n",
              "      <td>Commerce Secretary Wilbur Ross  who oversee th...</td>\n",
              "      <td>Commerce Secretary Wilbur Ross oversee US Cens...</td>\n",
              "      <td>Commerce Secretary Wilbur Ross oversee Census ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article  number    global_bias  \\\n",
              "0        0       5  From the Left   \n",
              "1        0       5  From the Left   \n",
              "2        0       5  From the Left   \n",
              "3        0       5  From the Left   \n",
              "4        0       5  From the Left   \n",
              "\n",
              "                                               title news_source  \\\n",
              "0  Trump Administration Drops Citizenship Questio...    HuffPost   \n",
              "1  Trump Administration Drops Citizenship Questio...    HuffPost   \n",
              "2  Trump Administration Drops Citizenship Questio...    HuffPost   \n",
              "3  Trump Administration Drops Citizenship Questio...    HuffPost   \n",
              "4  Trump Administration Drops Citizenship Questio...    HuffPost   \n",
              "\n",
              "                                          text_ascii  \\\n",
              "0  A very sad time for America when the Supreme C...   \n",
              "1  A very sad time for America when the Supreme C...   \n",
              "2  A very sad time for America when the Supreme C...   \n",
              "3  A very sad time for America when the Supreme C...   \n",
              "4  A very sad time for America when the Supreme C...   \n",
              "\n",
              "                                          text_paras  para_count  \\\n",
              "0  A very sad time for America when the Supreme C...           0   \n",
              "1  President Donald Trump spoke out Tuesday on hi...           1   \n",
              "2  A very sad time for America when the Supreme C...           2   \n",
              "3  ....to do whatever is necessary to bring this ...           3   \n",
              "4  Commerce Secretary Wilbur Ross, who oversees t...           4   \n",
              "\n",
              "                                    text_paras_lemma  \\\n",
              "0  a very sad time for America when the Supreme C...   \n",
              "1  President Donald Trump speak out Tuesday on  a...   \n",
              "2  a very sad time for America when the Supreme C...   \n",
              "3  .... to do whatever be necessary to bring this...   \n",
              "4  Commerce Secretary Wilbur Ross , who oversee t...   \n",
              "\n",
              "                               text_paras_misc_clean  \\\n",
              "0  a very sad time for America when the Supreme C...   \n",
              "1  President Donald Trump speak out Tuesday on  a...   \n",
              "2  a very sad time for America when the Supreme C...   \n",
              "3   to do whatever be necessary to bring this mos...   \n",
              "4  Commerce Secretary Wilbur Ross  who oversee th...   \n",
              "\n",
              "                                text_paras_stopwords  \\\n",
              "0  sad time America Supreme Court United States a...   \n",
              "1  President Donald Trump speak Tuesday administr...   \n",
              "2  sad time America Supreme Court United States a...   \n",
              "3  whatever necessary bring vital question import...   \n",
              "4  Commerce Secretary Wilbur Ross oversee US Cens...   \n",
              "\n",
              "                           text_paras_no_small_words  \n",
              "0  sad time America Supreme Court United States a...  \n",
              "1  President Donald Trump speak Tuesday administr...  \n",
              "2  sad time America Supreme Court United States a...  \n",
              "3  whatever necessary bring vital question import...  \n",
              "4  Commerce Secretary Wilbur Ross oversee Census ...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1p3rTqNn20K"
      },
      "source": [
        "df_no_nulls = df.dropna(subset=['text_paras_no_small_words']).reset_index()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "R5yfg3YsoKHC",
        "outputId": "3d70630a-9c61-41e3-92ac-f7b07588843a"
      },
      "source": [
        "df_no_nulls.sample(5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>article</th>\n",
              "      <th>number</th>\n",
              "      <th>global_bias</th>\n",
              "      <th>title</th>\n",
              "      <th>news_source</th>\n",
              "      <th>text_ascii</th>\n",
              "      <th>text_paras</th>\n",
              "      <th>para_count</th>\n",
              "      <th>text_paras_lemma</th>\n",
              "      <th>text_paras_misc_clean</th>\n",
              "      <th>text_paras_stopwords</th>\n",
              "      <th>text_paras_no_small_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>242</td>\n",
              "      <td>9</td>\n",
              "      <td>57</td>\n",
              "      <td>From the Right</td>\n",
              "      <td>Hundreds Arrested in Moscow Protests</td>\n",
              "      <td>Fox News (Online News)</td>\n",
              "      <td>Police in Moscow arrested more than 750 people...</td>\n",
              "      <td>CLICK HERE TO GET THE FOX NEWS APP</td>\n",
              "      <td>13</td>\n",
              "      <td>click here to get the FOX news APP</td>\n",
              "      <td>click here to get the FOX news APP</td>\n",
              "      <td>get FOX news APP</td>\n",
              "      <td>get FOX news APP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24772</th>\n",
              "      <td>26686</td>\n",
              "      <td>1038</td>\n",
              "      <td>3374</td>\n",
              "      <td>From the Left</td>\n",
              "      <td>House Committee to Vote on Holding Barr in Con...</td>\n",
              "      <td>New York Times (News)</td>\n",
              "      <td>WASHINGTON  The Trump administration ruled out...</td>\n",
              "      <td>The Judiciary Committees chairman, Representat...</td>\n",
              "      <td>13</td>\n",
              "      <td>the Judiciary Committees chairman , Representa...</td>\n",
              "      <td>the Judiciary Committees chairman  Representat...</td>\n",
              "      <td>Judiciary Committees chairman Representative J...</td>\n",
              "      <td>Judiciary Committees chairman Representative J...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5345</th>\n",
              "      <td>5926</td>\n",
              "      <td>202</td>\n",
              "      <td>856</td>\n",
              "      <td>From the Left</td>\n",
              "      <td>Supreme Court Rules Trump's Tax Returns Can Be...</td>\n",
              "      <td>Washington Post</td>\n",
              "      <td>In the other, the court said the restrictions ...</td>\n",
              "      <td>The court said Congress cannot seek the presid...</td>\n",
              "      <td>39</td>\n",
              "      <td>the court say Congress can not seek the presid...</td>\n",
              "      <td>the court say Congress can not seek the presid...</td>\n",
              "      <td>court Congress seek president information part...</td>\n",
              "      <td>court Congress seek president information part...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22291</th>\n",
              "      <td>23868</td>\n",
              "      <td>935</td>\n",
              "      <td>3128</td>\n",
              "      <td>From the Right</td>\n",
              "      <td>Healthcare Debate Moves to Main Spotlight</td>\n",
              "      <td>Washington Times</td>\n",
              "      <td>WASHINGTON  A federal judges ruling that the O...</td>\n",
              "      <td>Douglas Holtz-Eakin, top policy adviser to Rep...</td>\n",
              "      <td>10</td>\n",
              "      <td>Douglas Holtz - Eakin , top policy adviser to ...</td>\n",
              "      <td>Douglas Holtz  Eakin  top policy adviser to Re...</td>\n",
              "      <td>Douglas Holtz Eakin top policy adviser Republi...</td>\n",
              "      <td>Douglas Holtz Eakin top policy adviser Republi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10867</th>\n",
              "      <td>11723</td>\n",
              "      <td>450</td>\n",
              "      <td>1704</td>\n",
              "      <td>From the Left</td>\n",
              "      <td>DOJ: Ferguson Police Racially Biased</td>\n",
              "      <td>New York Times (News)</td>\n",
              "      <td>WASHINGTON  Ferguson, Mo., is a third white, b...</td>\n",
              "      <td>The report calls for city officials to acknowl...</td>\n",
              "      <td>13</td>\n",
              "      <td>the report call for city official to acknowled...</td>\n",
              "      <td>the report call for city official to acknowled...</td>\n",
              "      <td>report call city official acknowledge police d...</td>\n",
              "      <td>report call city official acknowledge police d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       index  article  number     global_bias  \\\n",
              "218      242        9      57  From the Right   \n",
              "24772  26686     1038    3374   From the Left   \n",
              "5345    5926      202     856   From the Left   \n",
              "22291  23868      935    3128  From the Right   \n",
              "10867  11723      450    1704   From the Left   \n",
              "\n",
              "                                                   title  \\\n",
              "218                 Hundreds Arrested in Moscow Protests   \n",
              "24772  House Committee to Vote on Holding Barr in Con...   \n",
              "5345   Supreme Court Rules Trump's Tax Returns Can Be...   \n",
              "22291          Healthcare Debate Moves to Main Spotlight   \n",
              "10867               DOJ: Ferguson Police Racially Biased   \n",
              "\n",
              "                  news_source  \\\n",
              "218    Fox News (Online News)   \n",
              "24772   New York Times (News)   \n",
              "5345          Washington Post   \n",
              "22291        Washington Times   \n",
              "10867   New York Times (News)   \n",
              "\n",
              "                                              text_ascii  \\\n",
              "218    Police in Moscow arrested more than 750 people...   \n",
              "24772  WASHINGTON  The Trump administration ruled out...   \n",
              "5345   In the other, the court said the restrictions ...   \n",
              "22291  WASHINGTON  A federal judges ruling that the O...   \n",
              "10867  WASHINGTON  Ferguson, Mo., is a third white, b...   \n",
              "\n",
              "                                              text_paras  para_count  \\\n",
              "218                   CLICK HERE TO GET THE FOX NEWS APP          13   \n",
              "24772  The Judiciary Committees chairman, Representat...          13   \n",
              "5345   The court said Congress cannot seek the presid...          39   \n",
              "22291  Douglas Holtz-Eakin, top policy adviser to Rep...          10   \n",
              "10867  The report calls for city officials to acknowl...          13   \n",
              "\n",
              "                                        text_paras_lemma  \\\n",
              "218                   click here to get the FOX news APP   \n",
              "24772  the Judiciary Committees chairman , Representa...   \n",
              "5345   the court say Congress can not seek the presid...   \n",
              "22291  Douglas Holtz - Eakin , top policy adviser to ...   \n",
              "10867  the report call for city official to acknowled...   \n",
              "\n",
              "                                   text_paras_misc_clean  \\\n",
              "218                   click here to get the FOX news APP   \n",
              "24772  the Judiciary Committees chairman  Representat...   \n",
              "5345   the court say Congress can not seek the presid...   \n",
              "22291  Douglas Holtz  Eakin  top policy adviser to Re...   \n",
              "10867  the report call for city official to acknowled...   \n",
              "\n",
              "                                    text_paras_stopwords  \\\n",
              "218                                     get FOX news APP   \n",
              "24772  Judiciary Committees chairman Representative J...   \n",
              "5345   court Congress seek president information part...   \n",
              "22291  Douglas Holtz Eakin top policy adviser Republi...   \n",
              "10867  report call city official acknowledge police d...   \n",
              "\n",
              "                               text_paras_no_small_words  \n",
              "218                                     get FOX news APP  \n",
              "24772  Judiciary Committees chairman Representative J...  \n",
              "5345   court Congress seek president information part...  \n",
              "22291  Douglas Holtz Eakin top policy adviser Republi...  \n",
              "10867  report call city official acknowledge police d...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n2wWg30xiq0"
      },
      "source": [
        "doc = df_no_nulls.text_paras.tolist()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnjHZ4lUm3og",
        "outputId": "8a713664-9f8c-4ae0-f15b-058a070295d5"
      },
      "source": [
        "model = Top2Vec(documents=doc, speed=\"learn\", workers=8)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-29 21:26:16,647 - top2vec - INFO - Pre-processing documents for training\n",
            "2020-11-29 21:26:19,018 - top2vec - INFO - Creating joint document/word embedding\n",
            "2020-11-29 21:28:43,947 - top2vec - INFO - Creating lower dimension embedding of documents\n",
            "2020-11-29 21:29:01,160 - top2vec - INFO - Finding dense areas of documents\n",
            "2020-11-29 21:29:04,073 - top2vec - INFO - Finding topics\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br-6lU3xnBmr",
        "outputId": "7b403fc0-08a1-42b8-871e-d543f0e8adac"
      },
      "source": [
        "model.get_num_topics()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4gad51B1JYl"
      },
      "source": [
        "topic_words, word_scores, topic_nums = model.get_topics()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHz4y-Iz7evY",
        "outputId": "9232cf52-14f7-4845-906b-8332214f6c3b"
      },
      "source": [
        "df_no_nulls.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25863, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhe8GiBMCChU"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Unbiased_news/Data/top2vec_model.pickle\", 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05e6zcJs5Mmg"
      },
      "source": [
        "docs, doc_topic_score, doc_ids = model.search_documents_by_topic(topic_num=5, num_docs=330, return_documents=True, reduced=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-cPbjmf71Sj",
        "outputId": "2ea7e9ad-9610-4e36-bc2f-f74dcb3f636d"
      },
      "source": [
        "help(model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on Top2Vec in module top2vec.Top2Vec object:\n",
            "\n",
            "class Top2Vec(builtins.object)\n",
            " |  Top2Vec\n",
            " |  \n",
            " |  Creates jointly embedded topic, document and word vectors.\n",
            " |  \n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  embedding_model: string\n",
            " |      This will determine which model is used to generate the document and\n",
            " |      word embeddings. The valid string options are:\n",
            " |  \n",
            " |          * doc2vec\n",
            " |          * universal-sentence-encoder\n",
            " |          * universal-sentence-encoder-multilingual\n",
            " |          * distiluse-base-multilingual-cased\n",
            " |  \n",
            " |      For large data sets and data sets with very unique vocabulary doc2vec\n",
            " |      could produce better results. This will train a doc2vec model from\n",
            " |      scratch. This method is language agnostic. However multiple languages\n",
            " |      will not be aligned.\n",
            " |  \n",
            " |      Using the universal sentence encoder options will be much faster since\n",
            " |      those are pre-trained and efficient models. The universal sentence\n",
            " |      encoder options are suggested for smaller data sets. They are also\n",
            " |      good options for large data sets that are in English or in languages\n",
            " |      covered by the multilingual model. It is also suggested for data sets\n",
            " |      that are multilingual.\n",
            " |  \n",
            " |      For more information on universal-sentence-encoder visit:\n",
            " |      https://tfhub.dev/google/universal-sentence-encoder/4\n",
            " |  \n",
            " |      For more information on universal-sentence-encoder-multilingual visit:\n",
            " |      https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n",
            " |  \n",
            " |      The distiluse-base-multilingual-cased pre-trained sentence transformer\n",
            " |      is suggested for multilingual datasets and languages that are not\n",
            " |      covered by the multilingual universal sentence encoder. The transformer\n",
            " |      is significantly slower than the universal sentence encoder options.\n",
            " |  \n",
            " |      For more informati ond istiluse-base-multilingual-cased visit:\n",
            " |      https://www.sbert.net/docs/pretrained_models.html\n",
            " |  \n",
            " |  embedding_model_path: string (Optional)\n",
            " |      Pre-trained embedding models will be downloaded automatically by\n",
            " |      default. However they can also be uploaded from a file that is in the\n",
            " |      location of embedding_model_path.\n",
            " |  \n",
            " |      Warning: the model at embedding_model_path must match the\n",
            " |      embedding_model parameter type.\n",
            " |  \n",
            " |  documents: List of str\n",
            " |      Input corpus, should be a list of strings.\n",
            " |  \n",
            " |  min_count: int (Optional, default 50)\n",
            " |      Ignores all words with total frequency lower than this. For smaller\n",
            " |      corpora a smaller min_count will be necessary.\n",
            " |  \n",
            " |  speed: string (Optional, default 'learn')\n",
            " |  \n",
            " |      This parameter is only used when using doc2vec as embedding_model.\n",
            " |  \n",
            " |      It will determine how fast the model takes to train. The\n",
            " |      fast-learn option is the fastest and will generate the lowest quality\n",
            " |      vectors. The learn option will learn better quality vectors but take\n",
            " |      a longer time to train. The deep-learn option will learn the best quality\n",
            " |      vectors but will take significant time to train. The valid string speed\n",
            " |      options are:\n",
            " |      \n",
            " |          * fast-learn\n",
            " |          * learn\n",
            " |          * deep-learn\n",
            " |  \n",
            " |  use_corpus_file: bool (Optional, default False)\n",
            " |  \n",
            " |      This parameter is only used when using doc2vec as embedding_model.\n",
            " |  \n",
            " |      Setting use_corpus_file to True can sometimes provide speedup for large\n",
            " |      datasets when multiple worker threads are available. Documents are\n",
            " |      still passed to the model as a list of str, the model will create a\n",
            " |      temporary corpus file for training.\n",
            " |  \n",
            " |  document_ids: List of str, int (Optional)\n",
            " |      A unique value per document that will be used for referring to\n",
            " |      documents in search results. If ids are not given to the model, the\n",
            " |      index of each document in the original corpus will become the id.\n",
            " |  \n",
            " |  keep_documents: bool (Optional, default True)\n",
            " |      If set to False documents will only be used for training and not saved\n",
            " |      as part of the model. This will reduce model size. When using search\n",
            " |      functions only document ids will be returned, not the actual documents.\n",
            " |  \n",
            " |  workers: int (Optional)\n",
            " |      The amount of worker threads to be used in training the model. Larger\n",
            " |      amount will lead to faster training.\n",
            " |  \n",
            " |  tokenizer: callable (Optional, default None)\n",
            " |      Override the default tokenization method. If None then\n",
            " |      gensim.utils.simple_preprocess will be used.\n",
            " |  \n",
            " |  verbose: bool (Optional, default True)\n",
            " |      Whether to print status data during training.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, documents, min_count=50, embedding_model='doc2vec', embedding_model_path=None, speed='learn', use_corpus_file=False, document_ids=None, keep_documents=True, workers=None, tokenizer=None, verbose=True)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  add_documents(self, documents, doc_ids=None)\n",
            " |      Update the model with new documents.\n",
            " |      \n",
            " |      The documents will be added to the current model without changing\n",
            " |      existing document, word and topic vectors. Topic sizes will be updated.\n",
            " |      \n",
            " |      If adding a large quantity of documents relative to the current model\n",
            " |      size, or documents containing a largely new vocabulary, a new model\n",
            " |      should be trained for best results.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      documents: List of str\n",
            " |      \n",
            " |      doc_ids: List of str, int (Optional)\n",
            " |      \n",
            " |          Only required if doc_ids were given to the original model.\n",
            " |      \n",
            " |          A unique value per document that will be used for referring to\n",
            " |          documents in search results. If ids are not given to the model, the\n",
            " |          index of each document in the model will become the id.\n",
            " |  \n",
            " |  change_to_download_embedding_model(self)\n",
            " |      Use automatic download to load embedding model used for training.\n",
            " |      Top2Vec will no longer try and load the embedding model from a file\n",
            " |      if a embedding_model path was previously added.\n",
            " |  \n",
            " |  delete_documents(self, doc_ids)\n",
            " |      Delete documents from current model.\n",
            " |      \n",
            " |      Warning: If document ids were not used in original model, deleting\n",
            " |      documents will change the indexes and therefore doc_ids.\n",
            " |      \n",
            " |      The documents will be deleted from the current model without changing\n",
            " |      existing document, word and topic vectors. Topic sizes will be updated.\n",
            " |      \n",
            " |      If deleting a large quantity of documents relative to the current model\n",
            " |      size a new model should be trained for best results.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      doc_ids: List of str, int\n",
            " |      \n",
            " |          A unique value per document that is used for referring to documents\n",
            " |          in search results. If ids were not given to the model, the index of\n",
            " |          each document in the model is the id.\n",
            " |  \n",
            " |  generate_topic_wordcloud(self, topic_num, background_color='black', reduced=False)\n",
            " |      Create a word cloud for a topic.\n",
            " |      \n",
            " |      A word cloud will be generated and displayed. The most semantically\n",
            " |      similar words to the topic will have the largest size, less similar\n",
            " |      words will be smaller. The size is determined using the cosine distance\n",
            " |      of the word vectors from the topic vector.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      topic_num: int\n",
            " |          The topic number to search.\n",
            " |      \n",
            " |      background_color : str (Optional, default='white')\n",
            " |          Background color for the word cloud image. Suggested options are:\n",
            " |              * white\n",
            " |              * black\n",
            " |      \n",
            " |      reduced: bool (Optional, default False)\n",
            " |          Original topics are used by default. If True the\n",
            " |          reduced topics will be used.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      A matplotlib plot of the word cloud with the topic number will be\n",
            " |      displayed.\n",
            " |  \n",
            " |  get_documents_topics(self, doc_ids, reduced=False)\n",
            " |      Get document topics.\n",
            " |      \n",
            " |      The topic of each document will be returned.\n",
            " |      \n",
            " |      The corresponding original topics are returned unless reduced=True,\n",
            " |      in which case the reduced topics will be returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      doc_ids: List of str, int\n",
            " |          A unique value per document that is used for referring to documents\n",
            " |          in search results. If ids were not given to the model, the index of\n",
            " |          each document in the model is the id.\n",
            " |      \n",
            " |      reduced: bool (Optional, default False)\n",
            " |          Original topics are returned by default. If True the\n",
            " |          reduced topics will be returned.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      topic_nums: array of int, shape(doc_ids)\n",
            " |          The topic number of the document corresponding to each doc_id.\n",
            " |      \n",
            " |      topic_score: array of float, shape(doc_ids)\n",
            " |          Semantic similarity of document to topic. The cosine similarity of\n",
            " |          the document and topic vector.\n",
            " |      \n",
            " |      topics_words: array of shape(num_topics, 50)\n",
            " |          For each topic the top 50 words are returned, in order\n",
            " |          of semantic similarity to topic.\n",
            " |      \n",
            " |          Example:\n",
            " |          [['data', 'deep', 'learning' ... 'artificial'],          <Topic 4>\n",
            " |          ['environment', 'warming', 'climate ... 'temperature']  <Topic 21>\n",
            " |          ...]\n",
            " |      \n",
            " |      word_scores: array of shape(num_topics, 50)\n",
            " |          For each topic the cosine similarity scores of the\n",
            " |          top 50 words to the topic are returned.\n",
            " |      \n",
            " |          Example:\n",
            " |          [[0.7132, 0.6473, 0.5700 ... 0.3455],  <Topic 4>\n",
            " |          [0.7818', 0.7671, 0.7603 ... 0.6769]  <Topic 21>\n",
            " |          ...]\n",
            " |  \n",
            " |  get_num_topics(self, reduced=False)\n",
            " |      Get number of topics.\n",
            " |      \n",
            " |      This is the number of topics Top2Vec has found in the data by default.\n",
            " |      If reduced is True, the number of reduced topics is returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      reduced: bool (Optional, default False)\n",
            " |          The number of original topics will be returned by default. If True\n",
            " |          will return the number of reduced topics, if hierarchical topic\n",
            " |          reduction has been performed.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      num_topics: int\n",
            " |  \n",
            " |  get_topic_hierarchy(self)\n",
            " |      Get the hierarchy of reduced topics. The mapping of each original topic\n",
            " |      to the reduced topics is returned.\n",
            " |      \n",
            " |      Hierarchical topic reduction must be performed before calling this\n",
            " |      method.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      hierarchy: list of ints\n",
            " |          Each index of the hierarchy corresponds to the topic number of a\n",
            " |          reduced topic. For each reduced topic the topic numbers of the\n",
            " |          original topics that were merged to create it are listed.\n",
            " |      \n",
            " |          Example:\n",
            " |          [[3]  <Reduced Topic 0> contains original Topic 3\n",
            " |          [2,4] <Reduced Topic 1> contains original Topics 2 and 4\n",
            " |          [0,1] <Reduced Topic 3> contains original Topics 0 and 1\n",
            " |          ...]\n",
            " |  \n",
            " |  get_topic_sizes(self, reduced=False)\n",
            " |      Get topic sizes.\n",
            " |      \n",
            " |      The number of documents most similar to each topic. Topics are\n",
            " |      in increasing order of size.\n",
            " |      \n",
            " |      The sizes of the original topics is returned unless reduced=True,\n",
            " |      in which case the sizes of the reduced topics will be returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      reduced: bool (Optional, default False)\n",
            " |          Original topic sizes are returned by default. If True the\n",
            " |          reduced topic sizes will be returned.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      topic_sizes: array of int, shape(num_topics)\n",
            " |          The number of documents most similar to the topic.\n",
            " |      topic_nums: array of int, shape(num_topics)\n",
            " |          The unique number of every topic will be returned.\n",
            " |  \n",
            " |  get_topics(self, num_topics=None, reduced=False)\n",
            " |      Get topics, ordered by decreasing size. All topics are returned\n",
            " |      if num_topics is not specified.\n",
            " |      \n",
            " |      The original topics found are returned unless reduced=True,\n",
            " |      in which case reduced topics will be returned.\n",
            " |      \n",
            " |      Each topic will consist of the top 50 semantically similar words\n",
            " |      to the topic. These are the 50 words closest to topic vector\n",
            " |      along with cosine similarity of each word from vector. The\n",
            " |      higher the score the more relevant the word is to the topic.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num_topics: int, (Optional)\n",
            " |          Number of topics to return.\n",
            " |      \n",
            " |      reduced: bool (Optional, default False)\n",
            " |          Original topics are returned by default. If True the\n",
            " |          reduced topics will be returned.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      topics_words: array of shape(num_topics, 50)\n",
            " |          For each topic the top 50 words are returned, in order\n",
            " |          of semantic similarity to topic.\n",
            " |          \n",
            " |          Example:\n",
            " |          [['data', 'deep', 'learning' ... 'artificial'],         <Topic 0>\n",
            " |          ['environment', 'warming', 'climate ... 'temperature']  <Topic 1>\n",
            " |          ...]\n",
            " |      \n",
            " |      word_scores: array of shape(num_topics, 50)\n",
            " |          For each topic the cosine similarity scores of the\n",
            " |          top 50 words to the topic are returned.\n",
            " |          \n",
            " |          Example:\n",
            " |          [[0.7132, 0.6473, 0.5700 ... 0.3455],  <Topic 0>\n",
            " |          [0.7818', 0.7671, 0.7603 ... 0.6769]   <Topic 1>\n",
            " |          ...]\n",
            " |      \n",
            " |      topic_nums: array of int, shape(num_topics)\n",
            " |          The unique number of every topic will be returned.\n",
            " |  \n",
            " |  hierarchical_topic_reduction(self, num_topics)\n",
            " |      Reduce the number of topics discovered by Top2Vec.\n",
            " |      \n",
            " |      The most representative topics of the corpus will be found, by\n",
            " |      iteratively merging each smallest topic to the most similar topic until\n",
            " |      num_topics is reached.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num_topics: int\n",
            " |          The number of topics to reduce to.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      hierarchy: list of ints\n",
            " |          Each index of hierarchy corresponds to the reduced topics, for each\n",
            " |          reduced topic the indexes of the original topics that were merged\n",
            " |          to create it are listed.\n",
            " |      \n",
            " |          Example:\n",
            " |          [[3]  <Reduced Topic 0> contains original Topic 3\n",
            " |          [2,4] <Reduced Topic 1> contains original Topics 2 and 4\n",
            " |          [0,1] <Reduced Topic 3> contains original Topics 0 and 1\n",
            " |          ...]\n",
            " |  \n",
            " |  save(self, file)\n",
            " |      Saves the current model to the specified file.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      file: str\n",
            " |          File where model will be saved.\n",
            " |  \n",
            " |  search_documents_by_documents(self, doc_ids, num_docs, doc_ids_neg=None, return_documents=True)\n",
            " |      Semantic similarity search of documents.\n",
            " |      \n",
            " |      The most semantically similar documents to the semantic combination of\n",
            " |      document ids provided will be returned. If negative document ids are\n",
            " |      provided, the documents will be semantically dissimilar to those\n",
            " |      document ids. Documents will be ordered by decreasing similarity. This\n",
            " |      method finds the closest document vectors to the provided documents\n",
            " |      averaged.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      doc_ids: List of int, str\n",
            " |          Unique ids of document. If ids were not given, the index of\n",
            " |          document in the original corpus.\n",
            " |      \n",
            " |      doc_ids_neg: (Optional) List of int, str\n",
            " |          Unique ids of document. If ids were not given, the index of\n",
            " |          document in the original corpus.\n",
            " |      \n",
            " |      num_docs: int\n",
            " |          Number of documents to return.\n",
            " |      \n",
            " |      return_documents: bool (Optional default True)\n",
            " |          Determines if the documents will be returned. If they were not\n",
            " |          saved in the model they will also not be returned.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      documents: (Optional) array of str, shape(num_docs)\n",
            " |          The documents in a list, the most similar are first.\n",
            " |      \n",
            " |          Will only be returned if the documents were saved and if\n",
            " |          return_documents is set to True.\n",
            " |      \n",
            " |      doc_scores: array of float, shape(num_docs)\n",
            " |          Semantic similarity of document to keywords. The cosine similarity\n",
            " |          of the document and average of keyword vectors.\n",
            " |      \n",
            " |      doc_ids: array of int, shape(num_docs)\n",
            " |          Unique ids of documents. If ids were not given to the model, the\n",
            " |          index of the document in the model will be returned.\n",
            " |  \n",
            " |  search_documents_by_keywords(self, keywords, num_docs, keywords_neg=None, return_documents=True)\n",
            " |      Semantic search of documents using keywords.\n",
            " |      \n",
            " |      The most semantically similar documents to the combination of the\n",
            " |      keywords will be returned. If negative keywords are provided, the\n",
            " |      documents will be semantically dissimilar to those words. Too many\n",
            " |      keywords or certain combinations of words may give strange results.\n",
            " |      This method finds an average vector(negative keywords are subtracted)\n",
            " |      of all the keyword vectors and returns the documents closest to the\n",
            " |      resulting vector.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keywords: List of str\n",
            " |          List of positive keywords being used for search of semantically\n",
            " |          similar documents.\n",
            " |      \n",
            " |      keywords_neg: List of str (Optional)\n",
            " |          List of negative keywords being used for search of semantically\n",
            " |          dissimilar documents.\n",
            " |      \n",
            " |      num_docs: int\n",
            " |          Number of documents to return.\n",
            " |      \n",
            " |      return_documents: bool (Optional default True)\n",
            " |          Determines if the documents will be returned. If they were not\n",
            " |          saved in the model they will also not be returned.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      documents: (Optional) array of str, shape(num_docs)\n",
            " |          The documents in a list, the most similar are first.\n",
            " |      \n",
            " |          Will only be returned if the documents were saved and if\n",
            " |          return_documents is set to True.\n",
            " |      \n",
            " |      doc_scores: array of float, shape(num_docs)\n",
            " |          Semantic similarity of document to keywords. The cosine similarity\n",
            " |          of the document and average of keyword vectors.\n",
            " |      \n",
            " |      doc_ids: array of int, shape(num_docs)\n",
            " |          Unique ids of documents. If ids were not given to the model, the\n",
            " |          index of the document in the model will be returned.\n",
            " |  \n",
            " |  search_documents_by_topic(self, topic_num, num_docs, return_documents=True, reduced=False)\n",
            " |      Get the most semantically similar documents to the topic.\n",
            " |      \n",
            " |      These are the documents closest to the topic vector. Documents are\n",
            " |      ordered by proximity to the topic vector. Successive documents in the\n",
            " |      list are less semantically similar to the topic.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      topic_num: int\n",
            " |          The topic number to search.\n",
            " |      \n",
            " |      num_docs: int\n",
            " |          Number of documents to return.\n",
            " |      \n",
            " |      return_documents: bool (Optional default True)\n",
            " |          Determines if the documents will be returned. If they were not\n",
            " |          saved in the model they will also not be returned.\n",
            " |      \n",
            " |      reduced: bool (Optional, default False)\n",
            " |          Original topics are used to search by default. If True the\n",
            " |          reduced topics will be used.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      documents: (Optional) array of str, shape(num_docs)\n",
            " |          The documents in a list, the most similar are first.\n",
            " |      \n",
            " |          Will only be returned if the documents were saved and if\n",
            " |          return_documents is set to True.\n",
            " |      \n",
            " |      doc_scores: array of float, shape(num_docs)\n",
            " |          Semantic similarity of document to topic. The cosine similarity of\n",
            " |          the document and topic vector.\n",
            " |      \n",
            " |      doc_ids: array of int, shape(num_docs)\n",
            " |          Unique ids of documents. If ids were not given to the model, the\n",
            " |          index of the document in the model will be returned.\n",
            " |  \n",
            " |  search_topics(self, keywords, num_topics, keywords_neg=None, reduced=False)\n",
            " |      Semantic search of topics using keywords.\n",
            " |      \n",
            " |      The most semantically similar topics to the combination of the keywords\n",
            " |      will be returned. If negative keywords are provided, the topics will be\n",
            " |      semantically dissimilar to those words. Topics will be ordered by\n",
            " |      decreasing similarity to the keywords. Too many keywords or certain\n",
            " |      combinations of words may give strange results. This method finds an\n",
            " |      average vector(negative keywords are subtracted) of all the keyword\n",
            " |      vectors and returns the topics closest to the resulting vector.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keywords: List of str\n",
            " |          List of positive keywords being used for search of semantically\n",
            " |          similar documents.\n",
            " |      \n",
            " |      keywords_neg: (Optional) List of str\n",
            " |          List of negative keywords being used for search of semantically\n",
            " |          dissimilar documents.\n",
            " |      \n",
            " |      num_topics: int\n",
            " |          Number of documents to return.\n",
            " |      \n",
            " |      reduced: bool (Optional, default False)\n",
            " |          Original topics are searched by default. If True the\n",
            " |          reduced topics will be searched.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      topics_words: array of shape (num_topics, 50)\n",
            " |          For each topic the top 50 words are returned, in order of semantic\n",
            " |          similarity to topic.\n",
            " |          \n",
            " |          Example:\n",
            " |          [['data', 'deep', 'learning' ... 'artificial'],           <Topic 0>\n",
            " |          ['environment', 'warming', 'climate ... 'temperature']    <Topic 1>\n",
            " |          ...]\n",
            " |      \n",
            " |      word_scores: array of shape (num_topics, 50)\n",
            " |          For each topic the cosine similarity scores of the top 50 words\n",
            " |          to the topic are returned.\n",
            " |          \n",
            " |          Example:\n",
            " |          [[0.7132, 0.6473, 0.5700 ... 0.3455],     <Topic 0>\n",
            " |          [0.7818', 0.7671, 0.7603 ... 0.6769]     <Topic 1>\n",
            " |          ...]\n",
            " |      \n",
            " |      topic_scores: array of float, shape(num_topics)\n",
            " |          For each topic the cosine similarity to the search keywords will be\n",
            " |          returned.\n",
            " |      \n",
            " |      topic_nums: array of int, shape(num_topics)\n",
            " |          The unique number of every topic will be returned.\n",
            " |  \n",
            " |  similar_words(self, keywords, num_words, keywords_neg=None)\n",
            " |      Semantic similarity search of words.\n",
            " |      \n",
            " |      The most semantically similar word to the combination of the keywords\n",
            " |      will be returned. If negative keywords are provided, the words will be\n",
            " |      semantically dissimilar to those words. Too many keywords or certain\n",
            " |      combinations of words may give strange results. This method finds an\n",
            " |      average vector(negative keywords are subtracted) of all the keyword\n",
            " |      vectors and returns the words closest to the resulting vector.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keywords: List of str\n",
            " |          List of positive keywords being used for search of semantically\n",
            " |          similar words.\n",
            " |      \n",
            " |      keywords_neg: List of str\n",
            " |          List of negative keywords being used for search of semantically\n",
            " |          dissimilar words.\n",
            " |      \n",
            " |      num_words: int\n",
            " |          Number of words to return.\n",
            " |      \n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      words: array of str, shape(num_words)\n",
            " |          The words in a list, the most similar are first.\n",
            " |      \n",
            " |      word_scores: array of float, shape(num_words)\n",
            " |          Semantic similarity of word to keywords. The cosine similarity of\n",
            " |          the word and average of keyword vectors.\n",
            " |  \n",
            " |  update_embedding_model_path(self, embedding_model_path)\n",
            " |      Update the path of the embedding model to be loaded. The model will\n",
            " |      no longer be downloaded but loaded from the path location.\n",
            " |      \n",
            " |      Warning: the model at embedding_model_path must match the\n",
            " |      embedding_model parameter type.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      embedding_model_path: Str\n",
            " |          Path to downloaded embedding model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  load(file) from builtins.type\n",
            " |      Load a pre-trained model from the specified file.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      file: str\n",
            " |          File where model will be loaded from.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpGk0SyO9lJ6",
        "outputId": "ab23035e-46df-4df5-a326-494debb65180"
      },
      "source": [
        "model.get_topic_sizes()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([755, 457, 454, 446, 348, 330, 315, 298, 294, 292, 287, 280, 273,\n",
              "        267, 249, 238, 233, 229, 217, 212, 202, 200, 192, 191, 185, 184,\n",
              "        176, 174, 173, 167, 164, 162, 162, 158, 156, 156, 153, 146, 140,\n",
              "        140, 139, 138, 137, 136, 135, 133, 132, 132, 130, 129, 127, 127,\n",
              "        126, 126, 123, 123, 120, 118, 117, 116, 116, 116, 115, 115, 115,\n",
              "        113, 113, 113, 111, 111, 109, 109, 108, 108, 107, 107, 106, 105,\n",
              "        105, 104, 103, 101, 100,  99,  96,  96,  96,  95,  94,  94,  94,\n",
              "         93,  93,  91,  91,  90,  90,  90,  90,  90,  89,  89,  88,  88,\n",
              "         87,  87,  87,  87,  86,  86,  86,  86,  85,  84,  84,  84,  84,\n",
              "         84,  84,  83,  83,  83,  82,  81,  81,  81,  81,  80,  79,  79,\n",
              "         79,  79,  79,  78,  78,  78,  78,  77,  77,  77,  77,  76,  75,\n",
              "         75,  75,  75,  75,  74,  74,  74,  74,  72,  71,  71,  71,  71,\n",
              "         70,  70,  69,  69,  69,  69,  68,  68,  68,  67,  67,  66,  66,\n",
              "         66,  66,  66,  65,  64,  64,  64,  64,  64,  64,  64,  63,  63,\n",
              "         63,  62,  62,  62,  62,  62,  62,  61,  61,  60,  60,  60,  60,\n",
              "         59,  59,  59,  59,  58,  58,  57,  57,  56,  56,  56,  56,  56,\n",
              "         55,  55,  55,  55,  55,  54,  53,  53,  53,  51,  51,  50,  49,\n",
              "         49,  49,  49,  48,  47,  47,  47,  46,  45,  45,  43,  43,  43,\n",
              "         42,  41,  40,  40,  39,  38,  38,  30]),\n",
              " array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
              "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
              "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
              "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
              "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
              "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
              "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
              "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
              "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
              "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
              "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
              "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
              "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
              "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
              "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
              "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
              "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
              "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
              "        234, 235, 236, 237, 238, 239, 240, 241]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2PCOMys3cLR",
        "outputId": "a88edd21-eb28-4325-a967-cd3c34ab11a6"
      },
      "source": [
        "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=48, num_docs=5)\n",
        "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
        "    print(f\"Document: {doc_id}, Score: {score}\")\n",
        "    print(\"-----------\")\n",
        "    print(doc)\n",
        "    print(\"-----------\")\n",
        "    print()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document: 18894, Score: 0.7890528440475464\n",
            "-----------\n",
            "She said Mr. Trump should be providing more money for the police, emergency medical workers and families in need of treatment.\n",
            "-----------\n",
            "\n",
            "Document: 22643, Score: 0.7260197997093201\n",
            "-----------\n",
            "Employees at those agencies deemed essential will continue working without pay, including many Transportation Security Administration workers dealing with the influx of holiday travelers. After every previous shutdown, Congress has passed legislation to retroactively pay employees.\n",
            "-----------\n",
            "\n",
            "Document: 9040, Score: 0.7209782004356384\n",
            "-----------\n",
            "Mr. Carney argued Wednesday that the law is providing consumers with more choices, not a disincentive to work.\n",
            "-----------\n",
            "\n",
            "Document: 22552, Score: 0.7207549810409546\n",
            "-----------\n",
            "Employees at those agencies deemed essential will continue working without pay, including many Transportation Security Administration workers dealing with the influx of holiday travelers. After every previous shutdown, Congress has passed legislation to retroactively pay employees.\n",
            "-----------\n",
            "\n",
            "Document: 8837, Score: 0.7204416990280151\n",
            "-----------\n",
            "The measure authorizes a 1 percent pay increase for civilian federal workers and U.S. military personnel. But in response to several examples of excess spending by federal agencies, the bill would put in place new limits on certain conferences, official travel and employee awards.\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}