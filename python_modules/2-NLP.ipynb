{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict \n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "import re\n",
    "import unicodedata\n",
    "from nlp_preprocessing import *\n",
    "from topic_modeling import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS, CountVectorizer\n",
    "import spacy\n",
    "\n",
    "sp_nlp = spacy.load('en')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>global_bias</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "      <th>link</th>\n",
       "      <th>news_title</th>\n",
       "      <th>news_source</th>\n",
       "      <th>news_link</th>\n",
       "      <th>bias</th>\n",
       "      <th>paras</th>\n",
       "      <th>authors</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>From the Left</td>\n",
       "      <td>Trump Administration Drops Citizenship Questio...</td>\n",
       "      <td>July 3rd, 2019</td>\n",
       "      <td>['The Trump Administration dropped plans to ad...</td>\n",
       "      <td>https://www.allsides.com/story/trump-administr...</td>\n",
       "      <td>Trump Responds After His Administration Drops ...</td>\n",
       "      <td>HuffPost</td>\n",
       "      <td>https://www.huffpost.com/entry/trump-citizensh...</td>\n",
       "      <td>Left</td>\n",
       "      <td>President Donald Trump spoke out Tuesday on hi...</td>\n",
       "      <td>['Antonia Blumberg', 'Huffpost Us', 'Reporter']</td>\n",
       "      <td>2019-07-03 08:13:05+05:30</td>\n",
       "      <td>“A very sad time for America when the Supreme ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>From the Right</td>\n",
       "      <td>Trump Administration Drops Citizenship Questio...</td>\n",
       "      <td>July 3rd, 2019</td>\n",
       "      <td>['The Trump Administration dropped plans to ad...</td>\n",
       "      <td>https://www.allsides.com/story/trump-administr...</td>\n",
       "      <td>Trump administration drops push for citizenshi...</td>\n",
       "      <td>Washington Times</td>\n",
       "      <td>https://www.washingtontimes.com/news/2019/jul/...</td>\n",
       "      <td>Lean Right</td>\n",
       "      <td>President Trump’s quest to add a citizenship q...</td>\n",
       "      <td>['The Washington Times Http', 'Stephen Dinan']</td>\n",
       "      <td>2019-07-02 00:00:00</td>\n",
       "      <td>President Trump‘s quest to add a citizenship q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>From the Left</td>\n",
       "      <td>Iran to Surpass Uranium Enrichment Breaching N...</td>\n",
       "      <td>July 7th, 2019</td>\n",
       "      <td>['On Sunday, Iranian officials said the countr...</td>\n",
       "      <td>https://www.allsides.com/story/iran-surpass-ur...</td>\n",
       "      <td>Iran Announces New Breach of Nuclear Deal Limi...</td>\n",
       "      <td>New York Times (News)</td>\n",
       "      <td>https://www.nytimes.com/2019/07/07/world/middl...</td>\n",
       "      <td>Lean Left</td>\n",
       "      <td>Iran said on Sunday that within hours it would...</td>\n",
       "      <td>['David D. Kirkpatrick', 'David E. Sanger']</td>\n",
       "      <td>2019-07-07 00:00:00</td>\n",
       "      <td>Iran said on Sunday that within hours it would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>From the Right</td>\n",
       "      <td>Iran to Surpass Uranium Enrichment Breaching N...</td>\n",
       "      <td>July 7th, 2019</td>\n",
       "      <td>['On Sunday, Iranian officials said the countr...</td>\n",
       "      <td>https://www.allsides.com/story/iran-surpass-ur...</td>\n",
       "      <td>Iran raises uranium enrichment as nuclear deal...</td>\n",
       "      <td>Washington Times</td>\n",
       "      <td>https://www.washingtontimes.com/news/2019/jul/...</td>\n",
       "      <td>Lean Right</td>\n",
       "      <td>Iran announced Sunday it will raise its level ...</td>\n",
       "      <td>['The Washington Times Http', 'Jon Gambrell', ...</td>\n",
       "      <td>2019-07-07 00:00:00</td>\n",
       "      <td>TEHRAN, Iran — Iran announced Sunday it will r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>From the Left</td>\n",
       "      <td>Social Media Summit Draws Wide Range of Coverage</td>\n",
       "      <td>July 12th, 2019</td>\n",
       "      <td>[\"The 'Social Media Summit' hosted by Presiden...</td>\n",
       "      <td>https://www.allsides.com/story/social-media-su...</td>\n",
       "      <td>Trump accuses social media companies of ‘terri...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>https://www.washingtonpost.com/technology/2019...</td>\n",
       "      <td>Lean Left</td>\n",
       "      <td>President Trump assailed Facebook, Google and ...</td>\n",
       "      <td>['Tony Romm', 'Senior Tech Policy Reporter']</td>\n",
       "      <td>2019-07-11 00:00:00</td>\n",
       "      <td>“Some of you are extraordinary. The crap you t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     global_bias                                              title  \\\n",
       "0       5   From the Left  Trump Administration Drops Citizenship Questio...   \n",
       "1       5  From the Right  Trump Administration Drops Citizenship Questio...   \n",
       "2      15   From the Left  Iran to Surpass Uranium Enrichment Breaching N...   \n",
       "3      15  From the Right  Iran to Surpass Uranium Enrichment Breaching N...   \n",
       "4      25   From the Left   Social Media Summit Draws Wide Range of Coverage   \n",
       "\n",
       "              date                                            summary  \\\n",
       "0   July 3rd, 2019  ['The Trump Administration dropped plans to ad...   \n",
       "1   July 3rd, 2019  ['The Trump Administration dropped plans to ad...   \n",
       "2   July 7th, 2019  ['On Sunday, Iranian officials said the countr...   \n",
       "3   July 7th, 2019  ['On Sunday, Iranian officials said the countr...   \n",
       "4  July 12th, 2019  [\"The 'Social Media Summit' hosted by Presiden...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.allsides.com/story/trump-administr...   \n",
       "1  https://www.allsides.com/story/trump-administr...   \n",
       "2  https://www.allsides.com/story/iran-surpass-ur...   \n",
       "3  https://www.allsides.com/story/iran-surpass-ur...   \n",
       "4  https://www.allsides.com/story/social-media-su...   \n",
       "\n",
       "                                          news_title            news_source  \\\n",
       "0  Trump Responds After His Administration Drops ...               HuffPost   \n",
       "1  Trump administration drops push for citizenshi...       Washington Times   \n",
       "2  Iran Announces New Breach of Nuclear Deal Limi...  New York Times (News)   \n",
       "3  Iran raises uranium enrichment as nuclear deal...       Washington Times   \n",
       "4  Trump accuses social media companies of ‘terri...        Washington Post   \n",
       "\n",
       "                                           news_link        bias  \\\n",
       "0  https://www.huffpost.com/entry/trump-citizensh...        Left   \n",
       "1  https://www.washingtontimes.com/news/2019/jul/...  Lean Right   \n",
       "2  https://www.nytimes.com/2019/07/07/world/middl...   Lean Left   \n",
       "3  https://www.washingtontimes.com/news/2019/jul/...  Lean Right   \n",
       "4  https://www.washingtonpost.com/technology/2019...   Lean Left   \n",
       "\n",
       "                                               paras  \\\n",
       "0  President Donald Trump spoke out Tuesday on hi...   \n",
       "1  President Trump’s quest to add a citizenship q...   \n",
       "2  Iran said on Sunday that within hours it would...   \n",
       "3  Iran announced Sunday it will raise its level ...   \n",
       "4  President Trump assailed Facebook, Google and ...   \n",
       "\n",
       "                                             authors  \\\n",
       "0    ['Antonia Blumberg', 'Huffpost Us', 'Reporter']   \n",
       "1     ['The Washington Times Http', 'Stephen Dinan']   \n",
       "2        ['David D. Kirkpatrick', 'David E. Sanger']   \n",
       "3  ['The Washington Times Http', 'Jon Gambrell', ...   \n",
       "4       ['Tony Romm', 'Senior Tech Policy Reporter']   \n",
       "\n",
       "                publish_date  \\\n",
       "0  2019-07-03 08:13:05+05:30   \n",
       "1        2019-07-02 00:00:00   \n",
       "2        2019-07-07 00:00:00   \n",
       "3        2019-07-07 00:00:00   \n",
       "4        2019-07-11 00:00:00   \n",
       "\n",
       "                                                text  \n",
       "0  “A very sad time for America when the Supreme ...  \n",
       "1  President Trump‘s quest to add a citizenship q...  \n",
       "2  Iran said on Sunday that within hours it would...  \n",
       "3  TEHRAN, Iran — Iran announced Sunday it will r...  \n",
       "4  “Some of you are extraordinary. The crap you t...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/data_NLP_round1.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each news article can contain slightly different unicode formatting, its best to convert everything to ascii format, to make it easier to work the data. All incomptabile characters will be converted or dropped. Since we are working with English, the hope is that a majority of the data is retained.\n",
    "**But we can come to this later to see how much data is being dropped.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_ascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Syria’s prime minister escaped a daring assass...</td>\n",
       "      <td>Syrias prime minister escaped a daring assassi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "277  Syria’s prime minister escaped a daring assass...   \n",
       "\n",
       "                                            text_ascii  \n",
       "277  Syrias prime minister escaped a daring assassi...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensuring everything is in ascii format and removing any wierd formatings.\n",
    "df['text_ascii'] = df.text.map(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('ascii'))\n",
    "df[['text','text_ascii']].sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing to work on\n",
    "\n",
    "1. Better cleaning process - Post lemma and pre lemma? what else??\n",
    "1. Compound term extraction - incl. punctuation separated & space separated\n",
    "1. Named entity extraction & linkage (eg: hong_kong vs hong kong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Into Paras\n",
    "\n",
    "Let's breakout each news article into paragraphs and expand this into a new dataframe.  \n",
    "These paragraphs will be treated as individual documents that will be used to vectorize & topic model. Post which, for a given overall news headline, each paragraph from the left & right bias will be compared to see pair up paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded = df[['number','global_bias','title','news_source','text_ascii']].copy(deep=True)\n",
    "\n",
    "# Splitting each para into a list of paras\n",
    "df_expanded['text_paras_list'] = df_expanded.text_ascii.str.split('\\n\\n')\n",
    "\n",
    "# Exploding the paragraphs into a dataframe, where each row has a paragraph\n",
    "df_expanded_col = pd.DataFrame(df_expanded.text_paras_list.explode())\n",
    "df_expanded_col.rename(columns={'text_paras_list':'text_paras'}, inplace=True)\n",
    "\n",
    "# Joining the exploded dataframe back, so that other metadata can be associated with it\n",
    "df_expanded = df_expanded.join(df_expanded_col,).reset_index()\n",
    "df_expanded.rename(columns={'index':'article'}, inplace=True)\n",
    "df_expanded.drop(columns='text_paras_list', inplace=True)\n",
    "\n",
    "# getting paragraph numbering\n",
    "df_expanded['para_count'] = df_expanded.groupby('article').cumcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatizing first helps preserve as much meaning of the word as possible, while separating out punctuation as needed. It also preserves entity names.  \n",
    "**Only need to link compound words somehow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 47s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_paras</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23653</th>\n",
       "      <td>I am ready and willing to support strong candi...</td>\n",
       "      <td>be ready and willing to support strong candid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26308</th>\n",
       "      <td>On Wednesday, Neal formally requested that the...</td>\n",
       "      <td>on Wednesday , Neal formally request that the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_paras  \\\n",
       "23653  I am ready and willing to support strong candi...   \n",
       "26308  On Wednesday, Neal formally requested that the...   \n",
       "\n",
       "                                                    temp  \n",
       "23653   be ready and willing to support strong candid...  \n",
       "26308  on Wednesday , Neal formally request that the ...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_expanded['text_paras_lemma'] = df_expanded.text_paras.map(spacy_lemmatization)\n",
    "df_expanded[['text_paras', 'text_paras_lemma']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                         text_paras  \\\n",
      "7124  Google didn't immediately respond to a request for comment, but the company has said its competitive edge comes from offering a product that billions of people choose to use each day. Alphabet's shares opened Tuesday up roughly 1%, ahead of the broader market, after The Wall Street Journal first reported news of the impending suit.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                           text_paras_lemma  \n",
      "7124  Google do not immediately respond to a request for comment , but the company have say  competitive edge come from offer a product that billion of people choose to use each day . Alphabet 's share open Tuesday up roughly 1 % , ahead of the broad market , after the Wall Street Journal first report news of the impending suit .  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df_expanded.sample()[['text_paras','text_paras_lemma']])\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Cleaning\n",
    "\n",
    "Misc. cleaning of the documents. Currently this involves just removing email addresses, website links & any non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_paras_lemma</th>\n",
       "      <th>text_paras_misc_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25126</th>\n",
       "      <td>President Donald Trump say late Wednesday that...</td>\n",
       "      <td>President Donald Trump say late Wednesday that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25920</th>\n",
       "      <td>the emergency declaration unsettle many lawmak...</td>\n",
       "      <td>the emergency declaration unsettle many lawmak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_paras_lemma  \\\n",
       "25126  President Donald Trump say late Wednesday that...   \n",
       "25920  the emergency declaration unsettle many lawmak...   \n",
       "\n",
       "                                   text_paras_misc_clean  \n",
       "25126  President Donald Trump say late Wednesday that...  \n",
       "25920  the emergency declaration unsettle many lawmak...  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_expanded['text_paras_misc_clean'] = df_expanded.text_paras_lemma.map(cleaning)\n",
    "df_expanded[['text_paras_lemma','text_paras_misc_clean']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_paras               All the components of the \"H-bomb\" were \"homemade,\" so North Korea could produce \"powerful nuclear weapons as many as it wants,\" the KCNA quoted Kim as saying.\n",
      "text_paras_misc_clean              all the component of the  h  bomb  be  homemade   so North Korea could produce  powerful nuclear weapon as many as  want   the KCNA quote Kim as say \n",
      "Name: 18300, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df_expanded.loc[18300,['text_paras','text_paras_misc_clean']])\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                text_paras  \\\n",
      "1916   Ms. Noel and Mr. Thomas were charged with conspiracy to defraud the United States and with making false records. They both surrendered to the F.B.I. on Tuesday morning and pleaded not guilty at a hearing in Federal District Court in Manhattan in the afternoon. Bail was set at $100,000 each.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                 text_paras_misc_clean  \n",
      "1916    Ms Noel and Mr Thomas be charge with conspiracy to defraud the United States and with make false record   both surrender to the FBI on Tuesday morning and plead not guilty at a hearing in Federal District Court in Manhattan in the afternoon  Bail be set at  100000 each   \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df_expanded.sample()[['text_paras','text_paras_misc_clean']])\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "custom_stop_words = ['ad', 'advertisement', '000', 'mr', 'ms', 'said', 'going', 'dont', 'think', 'know', 'want', 'like', 'im', 'thats', 'told', \\\n",
    "                     'lot', 'hes', 'really', 'say', 'added', 'come', 'great','newsletter','daily','sign','app',\\\n",
    "                    'click','app','inbox', 'latest', 'jr','everybody','`']\n",
    "\n",
    "df_expanded['text_paras_stopwords'] = df_expanded.text_paras_misc_clean.map(lambda x: remove_stopwords(x, custom_words=custom_stop_words))\n",
    "\n",
    "# df_expanded['text_paras_stopwords'] = df_expanded.text_paras_stopwords.map(lambda x: remove_stopwords(x, remove_words_list = [], \\\n",
    "#                                                                                                      custom_words = custom_stop_words))\n",
    "df_expanded[['text_paras_lemma','text_paras_stopwords']].sample(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['component', ''],\n",
       " ['`', ''],\n",
       " ['`', 'WORK_OF_ART'],\n",
       " ['h', 'WORK_OF_ART'],\n",
       " ['bomb', 'WORK_OF_ART'],\n",
       " ['`', ''],\n",
       " ['`', 'WORK_OF_ART'],\n",
       " ['`', 'WORK_OF_ART'],\n",
       " ['`', 'WORK_OF_ART'],\n",
       " ['homemade', 'WORK_OF_ART'],\n",
       " ['`', ''],\n",
       " ['`', ''],\n",
       " ['north', 'GPE'],\n",
       " ['korea', 'GPE'],\n",
       " ['could', ''],\n",
       " ['produce', ''],\n",
       " ['`', ''],\n",
       " ['`', ''],\n",
       " ['powerful', ''],\n",
       " ['nuclear', ''],\n",
       " ['weapon', ''],\n",
       " ['many', ''],\n",
       " ['want', ''],\n",
       " ['`', ''],\n",
       " ['`', ''],\n",
       " ['kcna', ''],\n",
       " ['quote', ''],\n",
       " ['kim', 'PERSON'],\n",
       " ['say', '']]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spacy_text = sp_nlp(df_expanded.loc[18300,'text_paras_stopwords'])\n",
    "# [[token.text, token.ent_type_] for token in spacy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nlp_round1['text_final'] = df_nlp_round1['text_stopwords']\n",
    "\n",
    "df_expanded['text_final'] = df_expanded['text_paras_stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4738"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_expanded['text_paras_stopwords'].str.contains('mr').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 100\n",
      "iteration: 2 of max_iter: 100\n",
      "iteration: 3 of max_iter: 100\n",
      "iteration: 4 of max_iter: 100\n",
      "iteration: 5 of max_iter: 100\n",
      "iteration: 6 of max_iter: 100\n",
      "iteration: 7 of max_iter: 100\n",
      "iteration: 8 of max_iter: 100\n",
      "iteration: 9 of max_iter: 100\n",
      "iteration: 10 of max_iter: 100\n",
      "iteration: 11 of max_iter: 100\n",
      "iteration: 12 of max_iter: 100\n",
      "iteration: 13 of max_iter: 100\n",
      "iteration: 14 of max_iter: 100\n",
      "iteration: 15 of max_iter: 100\n",
      "iteration: 16 of max_iter: 100\n",
      "iteration: 17 of max_iter: 100\n",
      "iteration: 18 of max_iter: 100\n",
      "iteration: 19 of max_iter: 100\n",
      "iteration: 20 of max_iter: 100\n",
      "iteration: 21 of max_iter: 100\n",
      "iteration: 22 of max_iter: 100\n",
      "iteration: 23 of max_iter: 100\n",
      "iteration: 24 of max_iter: 100\n",
      "iteration: 25 of max_iter: 100\n",
      "iteration: 26 of max_iter: 100\n",
      "iteration: 27 of max_iter: 100\n",
      "iteration: 28 of max_iter: 100\n",
      "iteration: 29 of max_iter: 100\n",
      "iteration: 30 of max_iter: 100\n",
      "iteration: 31 of max_iter: 100\n",
      "iteration: 32 of max_iter: 100\n",
      "iteration: 33 of max_iter: 100\n",
      "iteration: 34 of max_iter: 100\n",
      "iteration: 35 of max_iter: 100\n",
      "iteration: 36 of max_iter: 100\n",
      "iteration: 37 of max_iter: 100\n",
      "iteration: 38 of max_iter: 100\n",
      "iteration: 39 of max_iter: 100\n",
      "iteration: 40 of max_iter: 100\n",
      "iteration: 41 of max_iter: 100\n",
      "iteration: 42 of max_iter: 100\n",
      "iteration: 43 of max_iter: 100\n",
      "iteration: 44 of max_iter: 100\n",
      "iteration: 45 of max_iter: 100\n",
      "iteration: 46 of max_iter: 100\n",
      "iteration: 47 of max_iter: 100\n",
      "iteration: 48 of max_iter: 100\n",
      "iteration: 49 of max_iter: 100\n",
      "iteration: 50 of max_iter: 100\n",
      "iteration: 51 of max_iter: 100\n",
      "iteration: 52 of max_iter: 100\n",
      "iteration: 53 of max_iter: 100\n",
      "iteration: 54 of max_iter: 100\n",
      "iteration: 55 of max_iter: 100\n",
      "iteration: 56 of max_iter: 100\n",
      "iteration: 57 of max_iter: 100\n",
      "iteration: 58 of max_iter: 100\n",
      "iteration: 59 of max_iter: 100\n",
      "iteration: 60 of max_iter: 100\n",
      "iteration: 61 of max_iter: 100\n",
      "iteration: 62 of max_iter: 100\n",
      "iteration: 63 of max_iter: 100\n",
      "iteration: 64 of max_iter: 100\n",
      "iteration: 65 of max_iter: 100\n",
      "iteration: 66 of max_iter: 100\n",
      "iteration: 67 of max_iter: 100\n",
      "iteration: 68 of max_iter: 100\n",
      "iteration: 69 of max_iter: 100\n",
      "iteration: 70 of max_iter: 100\n",
      "iteration: 71 of max_iter: 100\n",
      "iteration: 72 of max_iter: 100\n",
      "iteration: 73 of max_iter: 100\n",
      "iteration: 74 of max_iter: 100\n",
      "iteration: 75 of max_iter: 100\n",
      "iteration: 76 of max_iter: 100\n",
      "iteration: 77 of max_iter: 100\n",
      "iteration: 78 of max_iter: 100\n",
      "iteration: 79 of max_iter: 100\n",
      "iteration: 80 of max_iter: 100\n",
      "iteration: 81 of max_iter: 100\n",
      "iteration: 82 of max_iter: 100\n",
      "iteration: 83 of max_iter: 100\n",
      "iteration: 84 of max_iter: 100\n",
      "iteration: 85 of max_iter: 100\n",
      "iteration: 86 of max_iter: 100\n",
      "iteration: 87 of max_iter: 100\n",
      "iteration: 88 of max_iter: 100\n",
      "iteration: 89 of max_iter: 100\n",
      "iteration: 90 of max_iter: 100\n",
      "iteration: 91 of max_iter: 100\n",
      "iteration: 92 of max_iter: 100\n",
      "iteration: 93 of max_iter: 100\n",
      "iteration: 94 of max_iter: 100\n",
      "iteration: 95 of max_iter: 100\n",
      "iteration: 96 of max_iter: 100\n",
      "iteration: 97 of max_iter: 100\n",
      "iteration: 98 of max_iter: 100\n",
      "iteration: 99 of max_iter: 100\n",
      "iteration: 100 of max_iter: 100\n",
      "Wall time: 9min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {'stop_words':'english','min_df': 10, 'max_df': 0.5, 'ngram_range':(1, 1),}\n",
    "\n",
    "tfidf = TfidfVectorizer(**params)\n",
    "review_word_matrix_tfidf = tfidf.fit_transform(df_expanded['text_final'])\n",
    "review_vocab_tfidf = tfidf.get_feature_names()\n",
    "\n",
    "lda_tfidf, score_tfidf, topic_matrix_tfidf, word_matrix_tfidf = lda_topic_modeling(review_word_matrix_tfidf, vocab = review_vocab_tfidf, n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring The Topic Models\n",
    "\n",
    "Let's take a look at the topic model to see what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "king, register, professor, nbc, water, barrett, university, harvard, clip, grassley, trudeau, todd, picture, lake, patrick, spillway, plenty, feeling, writer, positive, \n",
      "\n",
      "Topic 1\n",
      "trump, investigation, mr, committee, say, house, president, fbi, mueller, comey, attorney, counsel, flynn, impeachment, general, special, russia, report, probe, official, \n",
      "\n",
      "Topic 2\n",
      "percent, say, tax, health, year, job, care, pay, plan, billion, million, government, cut, rate, budget, worker, 000, insurance, economy, federal, \n",
      "\n",
      "Topic 3\n",
      "newsletter, daily, manage, sign, uranium, cumming, brunson, turkish, dowd, audience, opinion, conway, participation, hammer, yang, erdogan, liar, stuff, rosatom, quarter, \n",
      "\n",
      "Topic 4\n",
      "llc, copyright, 2020, times, click, permission, reprint, washington, buzz, nooyi, charles, word, post, 2006, warfare, pepsico, typically, drink, examiner, ag, \n",
      "\n",
      "Topic 5\n",
      "police, say, officer, protester, black, protest, city, people, video, mr, man, shooting, trump, walker, arrest, church, violence, death, group, shoot, \n",
      "\n",
      "Topic 6\n",
      "graham, reid, lindsey, harry, plane, space, mccain, rush, apartment, smart, mercer, mail, oath, vehicle, christopher, sen, guest, rich, say, car, \n",
      "\n",
      "Topic 7\n",
      "book, christie, appointee, brown, pugh, spicer, baltimore, loyalty, ginsburg, sean, fierce, wildstein, quiet, farr, audit, weather, belong, lane, carter, listen, \n",
      "\n",
      "Topic 8\n",
      "united, states, say, iran, mr, trade, american, china, war, deal, president, military, iraq, obama, sanction, nuclear, trump, world, foreign, force, \n",
      "\n",
      "Topic 9\n",
      "campaign, clinton, mr, democratic, candidate, trump, state, win, sander, republican, say, voter, election, party, vote, presidential, new, race, biden, president, \n",
      "\n",
      "Topic 10\n",
      "facebook, medium, social, company, northam, news, mr, twitter, update, tech, post, google, sandberg, say, video, hunt, registration, platform, celebrate, page, \n",
      "\n",
      "Topic 11\n",
      "say, people, bush, know, think, like, tell, thing, mr, family, good, great, life, want, president, work, time, look, year, add, \n",
      "\n",
      "Topic 12\n",
      "say, vote, senate, house, republican, democrats, republicans, mr, president, trump, think, congress, senator, leader, mcconnell, make, legislation, majority, come, party, \n",
      "\n",
      "Topic 13\n",
      "north, korea, kim, korean, summit, sen, rubio, tip, marco, independent, schultz, south, alaska, jong, direction, denuclearization, maine, paul, trump, contender, \n",
      "\n",
      "Topic 14\n",
      "border, say, immigrant, trump, immigration, wall, people, security, 000, administration, official, family, health, country, child, emergency, coronavirus, virus, illegal, president, \n",
      "\n",
      "Topic 15\n",
      "trump, president, house, white, say, mr, read, comment, official, news, russian, meeting, speak, tweet, adviser, putin, secretary, respond, continue, story, \n",
      "\n",
      "Topic 16\n",
      "court, say, law, justice, case, supreme, state, judge, department, federal, mr, decision, rule, email, use, president, clinton, information, legal, ruling, \n",
      "\n",
      "Topic 17\n",
      "contribute, fox, report, news, associated, press, app, click, hong, kong, los, bloomberg, angeles, david, elizabeth, business, mike, warren, ben, et, \n",
      "\n",
      "Topic 18\n",
      "say, schumer, mr, trump, israel, leader, president, pelosi, reporter, tell, minority, mnuchin, saudi, secretary, hamas, meeting, talk, treasury, negotiation, house, \n",
      "\n",
      "Topic 19\n",
      "weapon, attack, say, syrian, chemical, drone, military, assad, strike, iran, opinion, official, analysis, bomb, syria, terrorist, use, islamic, al, missile, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_words_for_all_topics(word_matrix_tfidf, 20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top words for each topics, there are a number of filler words which we could remove to make the topics a lot more senseful. Additionally, all numbers except for years can be removed too. Lastly, a way needs to be identified for detecting compound words, especially names of places, like Hong Kong, North America etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['000', 'mr', 'said', 'going', 'dont', 'think', 'know', 'want', 'like', 'im', 'thats', 'told', \\\n",
    "                     'lot', 'hes', 'really', 'say', 'added', 'come', 'great','newsletter','daily','sign','app',\\\n",
    "                    'click','app','inbox', 'latest', 'jr','everybody']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
